<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- SEO -->
    <title>Demystifying Transformers & LLMs | Shubham Kulkarni</title>
    <meta name="description" content="A technical deep dive into the Self-Attention mechanism. Understanding Query, Key, and Value vectors in modern Large Language Models.">
    <meta name="author" content="Shubham Kulkarni">
    <link rel="canonical" href="https://kulkarnishub377.github.io/sk/blog/generative-ai.html">
    <meta property="og:title" content="Demystifying Transformers: How LLMs Think">
    <meta property="og:description" content="The math behind the magic: Self-Attention explained.">
    <meta property="og:image" content="https://th.bing.com/th/id/R.e9f32b9ad5f2ba35b68ba6384e601669?rik=2KVzyToeNt9prw&riu=http%3a%2f%2fhekate.ai%2fwp-content%2fuploads%2f2024%2f01%2fgenerative-ai-la-gi.jpg&ehk=Z6iLVPLbSt%2fl%2fXfxTlul6l0GDD1NWr2llMJoIEc6LSE%3d&risl=&pid=ImgRaw&r=0">
    <meta property="og:url" content="https://kulkarnishub377.github.io/sk/blog/generative-ai.html">
    <meta property="og:type" content="article">

    <!-- Schema.org Authority -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://kulkarnishub377.github.io/sk/blog/generative-ai.html"
      },
      "headline": "Demystifying Transformers & LLMs",
      "description": "A technical breakdown of the Self-Attention mechanism, Positional Encodings, and why 'Attention is All You Need'.",
      "image": "https://th.bing.com/th/id/R.e9f32b9ad5f2ba35b68ba6384e601669?rik=2KVzyToeNt9prw&riu=http%3a%2f%2fhekate.ai%2fwp-content%2fuploads%2f2024%2f01%2fgenerative-ai-la-gi.jpg&ehk=Z6iLVPLbSt%2fl%2fXfxTlul6l0GDD1NWr2llMJoIEc6LSE%3d&risl=&pid=ImgRaw&r=0",
      "author": {
        "@type": "Person",
        "name": "Shubham Kulkarni",
        "url": "https://kulkarnishub377.github.io/sk/",
        "jobTitle": "AI Researcher"
      },
      "publisher": {
        "@type": "Person",
        "name": "Shubham Kulkarni"
      },
      "datePublished": "2026-02-18",
      "dateModified": "2026-02-18"
    }
    </script>

    <!-- Fonts & CSS -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&family=Outfit:wght@400;500;700;800&family=Libre+Baskerville:wght@400;700&display=swap" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
    <link rel="stylesheet" href="css/style.css">

    <style>
        /* --------------------------------------------------------------------------
           INTERNAL MAGAZINE STYLES
           -------------------------------------------------------------------------- */
        
        .article-content-wrapper {
            font-family: 'Inter', sans-serif;
            color: #2c3e50;
        }

        .article-content-wrapper p {
            font-size: 1.25rem;
            line-height: 1.85;
            margin-bottom: 2rem;
            color: #374151;
        }

        .article-content-wrapper h2 {
            font-family: 'Outfit', sans-serif;
            font-weight: 800;
            font-size: 2.25rem;
            margin-top: 4rem;
            margin-bottom: 1.5rem;
            color: #111827;
            position: relative;
            letter-spacing: -0.02em;
        }

        .article-content-wrapper h2::after {
            content: '';
            display: block;
            width: 80px;
            height: 6px;
            background: linear-gradient(90deg, #DB2777, transparent); /* Pink for Neural Nets */
            margin-top: 15px;
            border-radius: 4px;
        }

        .article-content-wrapper h3 {
            font-family: 'Outfit', sans-serif;
            font-weight: 700;
            font-size: 1.75rem;
            margin-top: 3rem;
            margin-bottom: 1.25rem;
            color: #BE185D; /* Darker Pink */
        }

        .dropcap::first-letter {
            font-family: 'Libre Baskerville', serif;
            font-size: 4.5rem;
            float: left;
            line-height: 0.85;
            margin-right: 1.25rem;
            margin-top: 0.2rem;
            color: #DB2777;
            font-weight: 700;
        }

        .highlight-box {
            background: #FDF2F8;
            border-left: 6px solid #DB2777;
            padding: 2.5rem;
            border-radius: 16px;
            margin: 3.5rem 0;
            position: relative;
            overflow: hidden;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
        }

        .highlight-box::before {
            content: '\f2db'; /* FontAwesome Microchip/Brain */
            font-family: 'Font Awesome 6 Free';
            font-weight: 900;
            position: absolute;
            top: -20px;
            right: -20px;
            font-size: 8rem;
            color: rgba(219, 39, 119, 0.05);
            transform: rotate(30deg);
        }

        .stat-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
            gap: 1.5rem;
            margin: 3.5rem 0;
        }

        .stat-card {
            background: #ffffff;
            padding: 2rem;
            border-radius: 16px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
            text-align: center;
            border: 1px solid rgba(0,0,0,0.05);
            transition: all 0.3s ease;
        }

        .stat-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1);
            border-color: rgba(219, 39, 119, 0.2);
        }

        .stat-value {
            font-size: 3rem;
            font-weight: 800;
            background: linear-gradient(135deg, #DB2777 0%, #BE185D 100%);
            -webkit-background-clip: text;
            background-clip: text;
            -webkit-text-fill-color: transparent;
            display: block;
            margin-bottom: 0.5rem;
        }

        .stat-label {
            font-size: 0.85rem;
            text-transform: uppercase;
            letter-spacing: 1.5px;
            color: #6B7280;
            font-weight: 700;
        }
        
        .toc-wrapper {
            background: #ffffff;
            padding: 2rem;
            border-radius: 16px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
            margin-bottom: 3rem;
            border: 1px solid #E5E7EB;
        }

        .toc-list { list-style: none; padding: 0; margin: 0; }
        .toc-list li { margin-bottom: 1rem; }
        .toc-list a { text-decoration: none; color: #4B5563; font-weight: 500; transition: color 0.2s ease; }
        .toc-list a:hover { color: #DB2777; }

        @media (max-width: 992px) {
            .hero-title { font-size: 2.75rem; }
            .stat-grid { grid-template-columns: 1fr; }
            .toc-wrapper { display: none; }
            .article-content-wrapper { padding: 0 0.5rem; }
        }
    </style>
</head>
<body>

    <!-- Nav -->
    <nav class="navbar navbar-expand-lg fixed-top navbar-premium">
        <div class="container">
            <a class="navbar-brand fw-bold d-flex align-items-center gap-2" href="index.html">
                <i class="fas fa-arrow-left text-secondary" style="font-size: 0.9rem;"></i>
                <span style="font-family: var(--font-mono); color: var(--text-primary);">Back to Blog</span>
            </a>
            
             <div class="ms-auto d-none d-md-block">
                <span class="badge bg-light text-dark border fw-medium px-3 py-2">
                    <i class="fas fa-clock me-2"></i> 11 min read
                </span>
            </div>
        </div>
    </nav>

    <!-- Header -->
    <header class="article-header pt-5 mt-5">
        <div class="container">
            <div class="row justify-content-center">
                <div class="col-lg-10 text-center">
                     <span class="hero-tag mb-3 d-inline-block text-white bg-danger border-danger" style="background-color: #DB2777 !important; border-color: #DB2777 !important;">Deep Learning</span>
                    <h1 class="hero-title display-4 fw-bolder mb-4">Demystifying Transformers: How LLMs Think</h1>
                    <p class="lead text-secondary mb-4 mx-auto" style="max-width: 800px; font-size: 1.35rem; line-height: 1.6;">
                        Beyond the hype: A technical breakdown of the Self-Attention mechanism, Positional Encodings, and why "Attention is All You Need".
                    </p>
                    
                    <div class="d-flex justify-content-center align-items-center gap-4 mt-5 text-muted">
                        <div class="d-flex align-items-center gap-2">
                            <img src="../photo_sk.jpg" alt="Shubham" class="rounded-circle shadow-sm" width="56" height="56">
                            <div class="text-start">
                                <span class="d-block fw-bold text-dark">Shubham Kulkarni</span>
                                <span class="small text-secondary">AI Researcher</span>
                            </div>
                        </div>
                        <div class="vr opacity-25"></div>
                        <div class="text-start">
                            <span class="d-block fw-bold text-dark">Published</span>
                            <time class="small text-secondary" datetime="2026-02-18">Feb 18, 2026</time>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Body -->
    <section class="py-5">
        <div class="container">
            <div class="row justify-content-center">
                
                <!-- Sidebar TOC -->
                <div class="col-lg-3 d-none d-lg-block">
                    <div class="sticky-top" style="top: 120px;">
                        <div class="toc-wrapper">
                            <h6 class="text-uppercase text-muted fw-bold mb-4 small" style="letter-spacing: 1px;">Contents</h6>
                            <ul class="toc-list">
                                <li><a href="#the-bottleneck">1. The RNN Bottleneck</a></li>
                                <li><a href="#self-attention">2. Self-Attention Mechanism</a></li>
                                <li><a href="#qkv-analogy">3. The Q, K, V Analogy</a></li>
                                <li><a href="#architecture">4. Transformer Architecture</a></li>
                                <li><a href="#future">5. Beyond Transformers</a></li>
                            </ul>
                        </div>
                    </div>
                </div>

                 <!-- Content -->
                <div class="col-lg-8 article-content-wrapper">
                    <img src="https://th.bing.com/th/id/R.e9f32b9ad5f2ba35b68ba6384e601669?rik=2KVzyToeNt9prw&riu=http%3a%2f%2fhekate.ai%2fwp-content%2fuploads%2f2024%2f01%2fgenerative-ai-la-gi.jpg&ehk=Z6iLVPLbSt%2fl%2fXfxTlul6l0GDD1NWr2llMJoIEc6LSE%3d&risl=&pid=ImgRaw&r=0" 
                         alt="Neural Network Visualization" class="img-fluid rounded-4 mb-5 shadow-sm w-100">

                    <p class="dropcap">
                        In 2017, the paper "Attention is All You Need" changed everything. Before that, we relied on RNNs and LSTMs, which processed text sequentially—word by word. This made training slow and context limited.
                    </p>
                    
                    <p>
                        Transformers introduced <strong>Parallelization</strong>. They could read an entire book at once (in theory), understanding the relationship between the first word and the last word instantly.
                    </p>

                    <div class="stat-grid">
                        <div class="stat-card">
                            <span class="stat-value">175B</span>
                            <span class="stat-label">Parameters (GPT-3)</span>
                        </div>
                        <div class="stat-card">
                            <span class="stat-value">2048</span>
                            <span class="stat-label">Context Window</span>
                        </div>
                        <div class="stat-card">
                            <span class="stat-value">O(N²)</span>
                            <span class="stat-label">Complexity</span>
                        </div>
                    </div>

                    <h2 id="the-bottleneck">1. The RNN Bottleneck</h2>
                    <p>
                        Recurrent Neural Networks (RNNs) suffer from the "Vanishing Gradient" problem. If you have a sentence like: "The <strong>cat</strong>, which was already full... [100 words] ... <strong>slept</strong>.", an RNN forgets "cat" by the time it reaches "slept".
                    </p>

                    <h2 id="self-attention">2. Self-Attention Mechanism</h2>
                    <p>
                        Self-attention allows the model to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.
                    </p>
                    
                    <div class="highlight-box">
                        <h4 class="h5 fw-bold mb-3"><i class="fas fa-search text-primary me-2" style="color: #DB2777 !important;"></i>Contextual Awareness</h4>
                        <p class="mb-0">
                            In the sentence "The animal didn't cross the street because <strong>it</strong> was too tired", what does "it" refer to? Self-attention assigns a higher weight to "animal" than "street" for the word "it".
                        </p>
                    </div>

                    <h2 id="qkv-analogy">3. The Query, Key, Value Analogy</h2>
                    <p>
                        Think of a database retrieval system.
                    </p>
                    <ul>
                        <li><strong>Query (Q):</strong> What you are looking for.</li>
                        <li><strong>Key (K):</strong> The labels in the database.</li>
                        <li><strong>Value (V):</strong> The actual content.</li>
                    </ul>
                    <p>
                        The attention score is calculated by the dot product of the Query and Key. If they align (high similarity), we retrieve more of the Value.
                    </p>

                    <div class="code-block-wrapper my-4">
                        <div class="d-flex justify-content-between border-bottom border-secondary border-opacity-25 pb-2 mb-3">
                            <span class="font-monospace small text-muted">attention.py</span>
                            <i class="fas fa-code text-muted"></i>
                        </div>
<pre><code class="language-python">import torch
import torch.nn.functional as F

def scaled_dot_product_attention(query, key, value):
    # 1. Matmul Q and K
    scores = torch.matmul(query, key.transpose(-2, -1))
    
    # 2. Scale
    d_k = query.size(-1)
    scores = scores / math.sqrt(d_k)
    
    # 3. Softmax
    attention_weights = F.softmax(scores, dim=-1)
    
    # 4. Matmul with V
    return torch.matmul(attention_weights, value)</code></pre>
                    </div>

                    <h2 id="architecture">4. The Architecture</h2>
                    <p>
                        The original Transformer has an Encoder and Decoder stack. GPT (Generative Pre-trained Transformer) only uses the **Decoder** stack to predict the next token.
                    </p>

                    <div class="code-block-wrapper my-4">
                        <div class="d-flex justify-content-between border-bottom border-secondary border-opacity-25 pb-2 mb-3">
                            <span class="font-monospace small text-muted">transformer.mermaid</span>
                            <i class="fas fa-code text-muted"></i>
                        </div>
<pre><code class="language-mermaid">graph TD
    Input[Input Embeddings] --> Pos[Positional Encoding]
    Pos --> Block1[Transformer Block 1]
    Block1 --> Block2[Transformer Block 2]
    Block2 --> BlockN[Transformer Block N]
    BlockN --> Norm[Layer Norm]
    Norm --> Linear[Linear Head]
    Linear --> Soft[Softmax]
    Soft --> Output[Next Token Probability]
    
    subgraph Transformer Block
        MultiHead[Multi-Head Attention] --> AddNorm1[Add & Norm]
        AddNorm1 --> FeedForward[Feed Forward Net]
        FeedForward --> AddNorm2[Add & Norm]
    end</code></pre>
                    </div>

                    <h2 id="future">5. Future: Efficient Attention?</h2>
                    <p>
                        The main limitation of Transformers is the O(N²) quadratic complexity required for the attention matrix. New research like **Mamba (State Space Models)** and **Ring Attention** aims to solve this, enabling 1M+ token context windows.
                    </p>

                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer style="background: var(--bg-surface); border-top: 1px solid var(--border-light);" class="py-5">
        <div class="container text-center">
            <h4 class="fw-bold mb-3" style="font-family: var(--font-heading);">Shubham Kulkarni</h4>
            <div class="d-flex justify-content-center gap-3 mb-4">
                <a href="#" class="text-secondary"><i class="fab fa-twitter fa-lg"></i></a>
                <a href="#" class="text-secondary"><i class="fab fa-github fa-lg"></i></a>
                <a href="#" class="text-secondary"><i class="fab fa-linkedin fa-lg"></i></a>
            </div>
            <p class="text-secondary small mb-4">Deciphering Intelligence.</p>
            <a href="index.html" class="btn btn-outline-dark rounded-pill px-4 hover-lift">Back to Blog</a>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({ startOnLoad: true, theme: 'neutral' });
    </script>
</body>
</html>
