<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Video Processing & Analysis with AI — Shubham Kulkarni</title>
  <meta name="description" content="Advanced techniques for video analytics including motion detection, activity recognition, and real-time processing using deep learning models and OpenCV.">
  <meta name="keywords" content="video processing, computer vision, AI, motion detection, activity recognition, OpenCV, deep learning, Shubham Kulkarni">
  <meta name="author" content="Shubham Kulkarni">
  <link rel="canonical" href="https://kulkarnishub377.github.io/sk/blog/video-processing.html">
  <meta property="og:type" content="article">
  <meta property="og:title" content="Video Processing & Analysis with AI">
  <meta property="og:description" content="Advanced techniques for video analytics using AI and computer vision.">
  <meta property="og:image" content="https://kulkarnishub377.github.io/sk/profile.jpg">
  <meta name="twitter:card" content="summary_large_image">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="stylesheet" href="../css/style.css">
  <link rel="stylesheet" href="../css/blog.css">
  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "headline": "Video Processing & Analysis with AI",
    "description": "Advanced techniques for video analytics including motion detection, activity recognition, and real-time processing using deep learning.",
    "author": {"@type": "Person", "name": "Shubham Kulkarni"},
    "publisher": {
      "@type": "Organization", 
      "name": "Shubham Kulkarni", 
      "logo": {"@type": "ImageObject", "url": "https://kulkarnishub377.github.io/sk/profile.jpg"}
    },
    "datePublished": "2024-12-08T00:00:00Z",
    "dateModified": "2024-12-08T00:00:00Z",
    "keywords": ["video processing", "computer vision", "AI", "deep learning", "OpenCV"]
  }
  </script>
</head>
<body>
  <nav class="navbar navbar-expand-lg">
    <div class="container">
      <a class="navbar-brand" href="../index.html">Shubham Kulkarni</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav ms-auto">
          <li class="nav-item"><a class="nav-link" href="../index.html">Home</a></li>
          <li class="nav-item"><a class="nav-link" href="../blog/index.html">Blog</a></li>
          <li class="nav-item"><a class="nav-link" href="../ai-projects.html">AI Projects</a></li>
          <li class="nav-item"><a class="nav-link" href="../index.html#contact">Contact</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <main class="py-5">
    <div class="container">
      <article class="blog-post-content mx-auto" style="max-width:900px;">
        <!-- Article Header -->
        <header class="blog-post-header">
          <h1 class="blog-post-title">Video Processing & Analysis with AI: From Motion Detection to Activity Recognition</h1>
          <div class="blog-post-meta">
            <span class="blog-card-category">AI & ML</span>
            <div class="blog-post-author">
              <img src="../profile.jpg" alt="Shubham Kulkarni" class="blog-post-author-avatar">
              <span><strong>Shubham Kulkarni</strong></span>
            </div>
            <span><i class="fas fa-calendar-alt"></i> December 8, 2024</span>
            <span class="blog-reading-time"><i class="fas fa-clock"></i> 15 min read</span>
          </div>
        </header>

        <!-- Social Share Buttons -->
        <div class="blog-social-share">
          <a href="#" class="social-share-btn share-twitter" data-share="twitter">
            <i class="fab fa-twitter"></i> Share on Twitter
          </a>
          <a href="#" class="social-share-btn share-linkedin" data-share="linkedin">
            <i class="fab fa-linkedin"></i> Share on LinkedIn
          </a>
          <a href="#" class="social-share-btn share-copy" data-share="copy">
            <i class="fas fa-link"></i> Copy Link
          </a>
        </div>

        <!-- Article Content -->
        <section>
          <h2>Introduction to Video Analytics</h2>
          <p>Video processing and analysis have become critical components of modern AI applications, from surveillance systems to autonomous vehicles. This comprehensive guide explores advanced techniques for extracting meaningful insights from video data using computer vision and deep learning.</p>
        </section>

        <section>
          <h2>Core Concepts in Video Processing</h2>
          <h3>1. Frame Extraction and Preprocessing</h3>
          <p>Video is essentially a sequence of images (frames). The first step in video processing involves extracting and preprocessing these frames:</p>
          <pre><code>import cv2
import numpy as np

def extract_frames(video_path, fps=30):
    """Extract frames from video at specified fps"""
    cap = cv2.VideoCapture(video_path)
    frame_rate = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(frame_rate / fps)
    
    frames = []
    count = 0
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
            
        if count % frame_interval == 0:
            # Preprocess frame
            frame = cv2.resize(frame, (640, 480))
            frame = cv2.GaussianBlur(frame, (5, 5), 0)
            frames.append(frame)
            
        count += 1
    
    cap.release()
    return frames</code></pre>

          <h3>2. Motion Detection</h3>
          <p>Background subtraction is a fundamental technique for detecting moving objects in video streams:</p>
          <pre><code>def detect_motion(video_path):
    """Detect motion using background subtraction"""
    cap = cv2.VideoCapture(video_path)
    
    # Initialize background subtractor
    bg_subtractor = cv2.createBackgroundSubtractorMOG2(
        history=500, 
        varThreshold=16, 
        detectShadows=True
    )
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # Apply background subtraction
        fg_mask = bg_subtractor.apply(frame)
        
        # Remove noise
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)
        
        # Find contours of moving objects
        contours, _ = cv2.findContours(
            fg_mask, 
            cv2.RETR_EXTERNAL, 
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        # Filter and draw bounding boxes
        for contour in contours:
            if cv2.contourArea(contour) > 500:  # Minimum area threshold
                x, y, w, h = cv2.boundingRect(contour)
                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        
        cv2.imshow('Motion Detection', frame)
        if cv2.waitKey(30) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()</code></pre>
        </section>

        <section>
          <h2>Object Tracking in Video</h2>
          <h3>Using Deep SORT Algorithm</h3>
          <p>For robust multi-object tracking, we use the Deep SORT (Simple Online and Realtime Tracking) algorithm which combines detection with appearance features:</p>
          <ul>
            <li><strong>Detection Phase:</strong> Use YOLOv8 or Faster R-CNN for object detection in each frame</li>
            <li><strong>Tracking Phase:</strong> Use Kalman filter for motion prediction and Hungarian algorithm for data association</li>
            <li><strong>Re-identification:</strong> Deep learning features for handling occlusions</li>
          </ul>
          
          <pre><code>from deep_sort_realtime.deepsort_tracker import DeepSort

def track_objects(video_path):
    """Track multiple objects across video frames"""
    tracker = DeepSort(max_age=30, n_init=3)
    detector = YOLO('yolov8n.pt')  # Load YOLOv8
    
    cap = cv2.VideoCapture(video_path)
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # Detect objects
        results = detector(frame)
        detections = []
        
        for r in results:
            boxes = r.boxes
            for box in boxes:
                x1, y1, x2, y2 = box.xyxy[0]
                conf = box.conf[0]
                cls = box.cls[0]
                
                detections.append([
                    [x1, y1, x2-x1, y2-y1],  # bbox
                    conf,  # confidence
                    int(cls)  # class
                ])
        
        # Update tracker
        tracks = tracker.update_tracks(detections, frame=frame)
        
        # Draw tracking results
        for track in tracks:
            if not track.is_confirmed():
                continue
            track_id = track.track_id
            bbox = track.to_ltrb()
            
            cv2.rectangle(frame, 
                         (int(bbox[0]), int(bbox[1])), 
                         (int(bbox[2]), int(bbox[3])), 
                         (0, 255, 0), 2)
            cv2.putText(frame, f'ID: {track_id}', 
                       (int(bbox[0]), int(bbox[1])-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        cv2.imshow('Tracking', frame)
        if cv2.waitKey(30) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()</code></pre>
        </section>

        <section>
          <h2>Activity Recognition</h2>
          <h3>Using 3D Convolutional Networks</h3>
          <p>Activity recognition involves understanding human actions from video sequences. We use 3D CNNs which can capture temporal information:</p>
          
          <h3>Architecture Overview</h3>
          <ul>
            <li><strong>Input:</strong> Sequences of N frames (e.g., 16 frames)</li>
            <li><strong>Network:</strong> 3D CNN (C3D, I3D, or SlowFast)</li>
            <li><strong>Output:</strong> Activity class (walking, running, jumping, etc.)</li>
          </ul>

          <pre><code>import torch
import torch.nn as nn
from torchvision.models.video import r3d_18

def recognize_activity(video_path, model_path):
    """Recognize human activities in video"""
    # Load pre-trained 3D ResNet
    model = r3d_18(pretrained=True)
    model.eval()
    
    # Activity labels (example)
    activities = [
        'walking', 'running', 'jumping', 
        'sitting', 'standing', 'waving'
    ]
    
    cap = cv2.VideoCapture(video_path)
    frames_buffer = []
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # Preprocess frame
        frame = cv2.resize(frame, (112, 112))
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames_buffer.append(frame)
        
        # Process when we have 16 frames
        if len(frames_buffer) == 16:
            # Prepare input tensor
            clip = np.array(frames_buffer)
            clip = torch.FloatTensor(clip).permute(3, 0, 1, 2)
            clip = clip.unsqueeze(0)  # Add batch dimension
            
            # Predict activity
            with torch.no_grad():
                output = model(clip)
                prediction = torch.argmax(output, dim=1).item()
            
            activity = activities[prediction]
            print(f"Detected Activity: {activity}")
            
            # Keep last 8 frames for sliding window
            frames_buffer = frames_buffer[8:]
    
    cap.release()</code></pre>
        </section>

        <section>
          <h2>Optical Flow Analysis</h2>
          <p>Optical flow captures the pattern of motion between consecutive frames, useful for understanding movement direction and speed:</p>
          
          <pre><code>def compute_optical_flow(video_path):
    """Compute dense optical flow"""
    cap = cv2.VideoCapture(video_path)
    ret, first_frame = cap.read()
    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)
    
    # Create HSV mask for visualization
    hsv_mask = np.zeros_like(first_frame)
    hsv_mask[..., 1] = 255
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow (Farneback method)
        flow = cv2.calcOpticalFlowFarneback(
            prev_gray, gray, None, 
            pyr_scale=0.5, levels=3, winsize=15,
            iterations=3, poly_n=5, poly_sigma=1.2, 
            flags=0
        )
        
        # Convert flow to polar coordinates
        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        
        # Visualize flow
        hsv_mask[..., 0] = angle * 180 / np.pi / 2
        hsv_mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_rgb = cv2.cvtColor(hsv_mask, cv2.COLOR_HSV2BGR)
        
        cv2.imshow('Optical Flow', flow_rgb)
        if cv2.waitKey(30) & 0xFF == ord('q'):
            break
        
        prev_gray = gray
    
    cap.release()
    cv2.destroyAllWindows()</code></pre>
        </section>

        <section>
          <h2>Real-World Applications</h2>
          <h3>1. Traffic Monitoring System</h3>
          <ul>
            <li>Vehicle counting and classification</li>
            <li>Speed estimation using optical flow</li>
            <li>Accident detection through anomaly detection</li>
            <li>Traffic density analysis</li>
          </ul>

          <h3>2. Surveillance and Security</h3>
          <ul>
            <li>Intrusion detection in restricted areas</li>
            <li>Crowd density estimation</li>
            <li>Suspicious activity recognition</li>
            <li>Face recognition and tracking</li>
          </ul>

          <h3>3. Sports Analytics</h3>
          <ul>
            <li>Player tracking and performance analysis</li>
            <li>Ball trajectory prediction</li>
            <li>Action recognition (goals, fouls, etc.)</li>
            <li>Team formation analysis</li>
          </ul>

          <h3>4. Healthcare Applications</h3>
          <ul>
            <li>Patient fall detection</li>
            <li>Gait analysis for rehabilitation</li>
            <li>Activity monitoring for elderly care</li>
            <li>Gesture recognition for assistive technology</li>
          </ul>
        </section>

        <section>
          <h2>Performance Optimization</h2>
          <h3>GPU Acceleration</h3>
          <p>For real-time video processing, GPU acceleration is essential:</p>
          <pre><code># Using CUDA with OpenCV
import cv2
import torch

# Enable GPU for PyTorch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Use GPU-accelerated OpenCV functions
gpu_frame = cv2.cuda_GpuMat()
gpu_frame.upload(frame)
# Process on GPU
gpu_result = cv2.cuda.resize(gpu_frame, (640, 480))
result = gpu_result.download()</code></pre>

          <h3>Frame Skipping</h3>
          <p>Process every Nth frame for non-critical applications to improve throughput</p>

          <h3>Multi-threading</h3>
          <p>Separate threads for video capture, processing, and display to maximize performance</p>
        </section>

        <section>
          <h2>Best Practices</h2>
          <ol>
            <li><strong>Choose the Right Model:</strong> Balance accuracy and speed based on application requirements</li>
            <li><strong>Preprocessing:</strong> Normalize frames, apply noise reduction, and ensure consistent input size</li>
            <li><strong>Batch Processing:</strong> Process multiple frames together for better GPU utilization</li>
            <li><strong>Memory Management:</strong> Release video capture resources properly to avoid memory leaks</li>
            <li><strong>Error Handling:</strong> Implement robust error handling for corrupted frames or video files</li>
            <li><strong>Benchmarking:</strong> Measure FPS and latency to ensure real-time performance</li>
          </ol>
        </section>

        <section>
          <h2>Tools and Libraries</h2>
          <div class="blog-tags">
            <span class="blog-tag">OpenCV</span>
            <span class="blog-tag">PyTorch</span>
            <span class="blog-tag">TensorFlow</span>
            <span class="blog-tag">YOLO</span>
            <span class="blog-tag">Deep SORT</span>
            <span class="blog-tag">MediaPipe</span>
            <span class="blog-tag">FFmpeg</span>
            <span class="blog-tag">CUDA</span>
            <span class="blog-tag">NumPy</span>
            <span class="blog-tag">Python</span>
          </div>
        </section>

        <section>
          <h2>Conclusion</h2>
          <p>Video processing and analysis with AI opens up countless possibilities across various domains. From surveillance to healthcare, the techniques covered in this article provide a foundation for building sophisticated video analytics applications. The key is to understand your specific requirements and choose the right combination of algorithms and optimizations.</p>
        </section>

        <section class="mt-4 bg-light p-3 rounded">
          <h4>Need Help with Video Analytics?</h4>
          <p>If you're working on video processing projects or need assistance implementing computer vision solutions, <a href="../index.html#contact">get in touch</a>! I specialize in building production-ready video analytics systems.</p>
        </section>

      </article>
    </div>
  </main>

  <footer class="text-center py-4">
    <p class="mb-0">© <span id="copyright-year">2024</span> Shubham Kulkarni</p>
  </footer>
  
  <script src="../js/script.js"></script>
  <script src="../js/blog.js"></script>
  <script>try{document.getElementById('copyright-year').textContent=new Date().getFullYear()}catch(e){};</script>
</body>
</html>
