<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>VIDES: Vehicle Image Detection System — Shubham Kulkarni</title>
  <meta name="description" content="Production-grade vehicle detection and tracking system for building perfect AI training datasets using YOLO, OpenCV, and advanced multi-object tracking.">
  <meta name="keywords" content="vehicle detection, VIDES, YOLO, ByteTrack, computer vision, AI training datasets, RTSP streams, Shubham Kulkarni">
  <meta name="author" content="Shubham Kulkarni">
  <link rel="canonical" href="https://kulkarnishub377.github.io/sk/blog/vehicle-detection-system.html">
  <meta property="og:type" content="article">
  <meta property="og:title" content="VIDES: Vehicle Image Detection System">
  <meta property="og:description" content="Production-grade vehicle detection and tracking system with multi-camera support and advanced quality control.">
  <meta property="og:image" content="https://kulkarnishub377.github.io/sk/profile.jpg">
  <meta name="twitter:card" content="summary_large_image">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="stylesheet" href="../css/style.css">
  <link rel="stylesheet" href="../css/blog.css">
  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "headline": "VIDES: Vehicle Image Detection System",
    "description": "Production-grade vehicle detection and tracking system for building perfect AI training datasets.",
    "author": {"@type": "Person", "name": "Shubham Kulkarni"},
    "publisher": {
      "@type": "Organization", 
      "name": "Shubham Kulkarni", 
      "logo": {"@type": "ImageObject", "url": "https://kulkarnishub377.github.io/sk/profile.jpg"}
    },
    "datePublished": "2024-12-08T00:00:00Z",
    "dateModified": "2024-12-08T00:00:00Z",
    "keywords": ["vehicle detection", "YOLO", "computer vision", "ByteTrack", "AI training", "RTSP"]
  }
  </script>
</head>
<body>
  <nav class="navbar navbar-expand-lg">
    <div class="container">
      <a class="navbar-brand" href="../index.html">Shubham Kulkarni</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav ms-auto">
          <li class="nav-item"><a class="nav-link" href="../index.html">Home</a></li>
          <li class="nav-item"><a class="nav-link" href="../blog/index.html">Blog</a></li>
          <li class="nav-item"><a class="nav-link" href="../ai-projects.html">AI Projects</a></li>
          <li class="nav-item"><a class="nav-link" href="../index.html#contact">Contact</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <main class="py-5">
    <div class="container">
      <article class="blog-post-content mx-auto" style="max-width:900px;">
        <!-- Article Header -->
        <header class="blog-post-header">
          <h1 class="blog-post-title">VIDES: Professional Vehicle Image Detection System</h1>
          <div class="blog-post-meta">
            <span class="blog-card-category">AI & ML</span>
            <div class="blog-post-author">
              <img src="../profile.jpg" alt="Shubham Kulkarni" class="blog-post-author-avatar">
              <span><strong>Shubham Kulkarni</strong></span>
            </div>
            <span><i class="fas fa-calendar-alt"></i> December 8, 2024</span>
            <span class="blog-reading-time"><i class="fas fa-clock"></i> 15 min read</span>
          </div>
        </header>

        <!-- Social Share Buttons -->
        <div class="blog-social-share">
          <a href="#" class="social-share-btn share-twitter" data-share="twitter">
            <i class="fab fa-twitter"></i> Share on Twitter
          </a>
          <a href="#" class="social-share-btn share-linkedin" data-share="linkedin">
            <i class="fab fa-linkedin"></i> Share on LinkedIn
          </a>
          <a href="#" class="social-share-btn share-copy" data-share="copy">
            <i class="fas fa-link"></i> Copy Link
          </a>
        </div>

        <!-- Article Content -->
        <section>
          <h2>Introduction to VIDES</h2>
          <p>The Vehicle Image Detection System (VIDES) is a production-grade vehicle detection and tracking system designed specifically for building high-quality AI training datasets. This comprehensive system handles multiple RTSP camera streams simultaneously, employs advanced tracking algorithms to prevent duplicate captures, and implements sophisticated quality control mechanisms to ensure every saved image is perfect for machine learning model training.</p>
        </section>

        <section>
          <h2>System Architecture</h2>
          <h3>Modular Design</h3>
          <p>VIDES follows a clean, modular architecture with clear separation of concerns:</p>
          <pre><code>├── main.py              # System orchestrator
├── config.py            # Configuration management
├── stream_reader.py     # RTSP stream handling
├── roi_manager.py       # ROI management
├── tracker.py           # Advanced vehicle tracking
└── utils.py             # Utility functions</code></pre>
          
          <p>This architecture ensures maintainability, scalability, and ease of testing while providing production-grade reliability.</p>
        </section>

        <section>
          <h2>Core Features</h2>
          <h3>1. Multi-Camera Support</h3>
          <p>VIDES can simultaneously process multiple RTSP streams from different cameras, making it ideal for large-scale surveillance or traffic monitoring systems. Each camera operates independently with its own configuration:</p>
          <pre><code>CAMERAS = {
    "overview": "rtsp://admin:password@192.168.1.100:554/stream1",
    "anpr": "rtsp://admin:password@192.168.1.101:554/stream1",
    "ptz": "rtsp://admin:password@192.168.1.102:554/stream1"
}</code></pre>

          <h3>2. Advanced Vehicle Tracking</h3>
          <p>The system uses ByteTrack combined with Kalman filtering to achieve zero-duplicate tracking. Each vehicle is assigned a unique ID and tracked throughout its journey in the frame:</p>
          <ul>
            <li><strong>ByteTrack Algorithm:</strong> Multi-object tracking with ID persistence</li>
            <li><strong>Kalman Filtering:</strong> Smooth trajectory prediction for handling occlusions</li>
            <li><strong>IoU Association:</strong> Match detections to existing tracks</li>
            <li><strong>Lost Track Management:</strong> Automatic cleanup of stale tracks after 30 frames</li>
          </ul>

          <h3>3. ROI-Based Filtering</h3>
          <p>Users can draw custom polygon regions of interest (ROI) for each camera. Only vehicles with their center point inside the ROI are tracked and saved, eliminating irrelevant detections:</p>
          <ul>
            <li>Interactive ROI drawing with mouse clicks</li>
            <li>Support for complex polygon shapes</li>
            <li>Per-camera ROI configuration saved to JSON</li>
            <li>Real-time visualization of active ROIs</li>
          </ul>

          <h3>4. Quality Control</h3>
          <p>VIDES implements rigorous quality checks to ensure only high-quality images are saved for training:</p>
          <ul>
            <li><strong>Blur Detection:</strong> Laplacian variance analysis (threshold: 100+)</li>
            <li><strong>Brightness Validation:</strong> Ensures proper illumination (30-220 range)</li>
            <li><strong>Stationary Vehicle Filtering:</strong> Rejects parked vehicles with minimal movement</li>
            <li><strong>Resolution Check:</strong> Validates minimum vehicle size requirements</li>
          </ul>
        </section>

        <section>
          <h2>Implementation Details</h2>
          <h3>Detection Pipeline</h3>
          <p>The system uses a custom-trained YOLO model for vehicle detection with the following classes:</p>
          <pre><code># Custom Vehicle Classes
VEHICLE_CLASSES = [
    'auto_rickshaw',
    'bike',
    'bus',
    'car',
    'mini_bus',
    'tractor',
    'truck'
]

# YOLO Configuration
CONFIDENCE_THRESHOLD = 0.5
IOU_THRESHOLD = 0.45
MODEL_PATH = 'bestv4.pt'</code></pre>

          <h3>Multi-Threading Architecture</h3>
          <p>For optimal performance, VIDES uses a multi-threaded approach:</p>
          <pre><code>class StreamReader(Thread):
    def __init__(self, stream_url, camera_id):
        super().__init__(daemon=True)
        self.stream_url = stream_url
        self.camera_id = camera_id
        self.latest_frame = None
        self.lock = Lock()
        
    def run(self):
        cap = cv2.VideoCapture(self.stream_url)
        while self.running:
            ret, frame = cap.read()
            if ret:
                with self.lock:
                    self.latest_frame = frame
            else:
                self.reconnect()
                
    def get_frame(self):
        with self.lock:
            return self.latest_frame.copy() if self.latest_frame is not None else None</code></pre>
        </section>

        <section>
          <h2>Advanced Capabilities</h2>
          <h3>Speed Estimation</h3>
          <p>VIDES can estimate vehicle speed in real-time using pixel displacement and calibrated distance:</p>
          <pre><code>def estimate_speed(track_id, current_pos, previous_pos, fps):
    # Calculate pixel displacement
    dx = current_pos[0] - previous_pos[0]
    dy = current_pos[1] - previous_pos[1]
    pixel_distance = np.sqrt(dx**2 + dy**2)
    
    # Convert to real-world distance
    real_distance = pixel_distance / PIXELS_PER_METER
    
    # Calculate speed (m/s to km/h)
    speed_mps = real_distance * fps
    speed_kmh = speed_mps * 3.6
    
    return speed_kmh</code></pre>

          <h3>Direction Detection</h3>
          <p>The system provides 8-way direction classification for vehicle movement:</p>
          <ul>
            <li>North, South, East, West</li>
            <li>Northeast, Northwest, Southeast, Southwest</li>
            <li>Based on trajectory analysis over multiple frames</li>
          </ul>

          <h3>Trajectory Analysis</h3>
          <p>Vehicle paths are smoothed and analyzed for pattern recognition:</p>
          <ul>
            <li>Path smoothing using moving average</li>
            <li>Turn detection and classification</li>
            <li>Lane change identification</li>
            <li>Abnormal behavior detection</li>
          </ul>
        </section>

        <section>
          <h2>Dataset Generation</h2>
          <h3>Output Structure</h3>
          <p>VIDES organizes captured images in a clean, camera-wise folder structure:</p>
          <pre><code>images/
├── frame/          # Full frame captures
│   ├── overview/
│   │   ├── frame_0.jpg
│   │   ├── frame_1.jpg
│   │   └── ...
│   ├── anpr/
│   └── ptz/
└── cropped/        # Vehicle crops
    ├── overview/
    │   ├── frame_0.jpg
    │   ├── frame_1.jpg
    │   └── ...
    ├── anpr/
    └── ptz/</code></pre>

          <h3>Image Quality Settings</h3>
          <pre><code># Maximum quality for training datasets
JPEG_QUALITY = 100
IMAGE_FORMAT = 'jpg'  # or 'png' for lossless

# Alternative: PNG for lossless compression
USE_PNG = False  # Set to True for maximum quality</code></pre>
        </section>

        <section>
          <h2>Real-World Applications</h2>
          <h3>1. Traffic Monitoring Systems</h3>
          <ul>
            <li>Highway traffic analysis and counting</li>
            <li>Intersection vehicle flow monitoring</li>
            <li>Traffic violation detection</li>
            <li>Congestion analysis</li>
          </ul>

          <h3>2. Dataset Creation for AI Training</h3>
          <ul>
            <li>Building large-scale vehicle detection datasets</li>
            <li>Creating annotated training data for YOLO models</li>
            <li>Gathering diverse vehicle type samples</li>
            <li>Collecting different lighting and weather conditions</li>
          </ul>

          <h3>3. Smart City Infrastructure</h3>
          <ul>
            <li>Parking lot management</li>
            <li>Automatic Number Plate Recognition (ANPR) integration</li>
            <li>Security and surveillance</li>
            <li>Emergency vehicle detection</li>
          </ul>

          <h3>4. Research Applications</h3>
          <ul>
            <li>Transportation research</li>
            <li>Urban planning studies</li>
            <li>Environmental impact analysis</li>
            <li>Behavioral pattern research</li>
          </ul>
        </section>

        <section>
          <h2>Performance Optimization</h2>
          <h3>For Maximum FPS</h3>
          <pre><code># Configuration for speed
BUFFER_SIZE = 1           # Lower latency
DEVICE = "0"              # Use GPU
ENABLE_QUALITY_CHECK = False  # Skip quality validation
RESIZE_WIDTH = 640        # Lower resolution</code></pre>

          <h3>For Best Quality</h3>
          <pre><code># Configuration for quality
MIN_BLUR_THRESHOLD = 150   # Stricter blur check
ENABLE_QUALITY_CHECK = True
USE_PNG = True            # Lossless format
RESIZE_WIDTH = 1920       # Full HD resolution</code></pre>

          <h3>Multi-Threading Benefits</h3>
          <ul>
            <li>Each camera has a dedicated capture thread</li>
            <li>Minimal buffer size for low latency (latest frame always available)</li>
            <li>Lock-free reads for maximum throughput</li>
            <li>Asynchronous image saving doesn't block processing</li>
          </ul>
        </section>

        <section>
          <h2>Statistics and Monitoring</h2>
          <p>VIDES provides comprehensive statistics for system monitoring:</p>
          <h3>Real-Time Metrics</h3>
          <ul>
            <li>FPS per camera stream</li>
            <li>Active vehicle tracks</li>
            <li>Total vehicles detected</li>
            <li>Frames saved count</li>
            <li>Rejection statistics</li>
            <li>Processing latency</li>
          </ul>

          <h3>Logging System</h3>
          <pre><code># Detailed logging to file and console
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('vehicle_capture.log'),
        logging.StreamHandler()
    ]
)</code></pre>
        </section>

        <section>
          <h2>Best Practices</h2>
          <ol>
            <li><strong>ROI Placement:</strong> Draw tight ROIs around lanes of interest for better accuracy</li>
            <li><strong>Camera Angles:</strong> Prefer frontal or rear views for better vehicle recognition</li>
            <li><strong>Lighting:</strong> Ensure adequate lighting; use IR cameras for night operations</li>
            <li><strong>Storage:</strong> Use SSD storage for fast image write operations</li>
            <li><strong>Network:</strong> Ensure stable network connection for RTSP streams</li>
            <li><strong>Calibration:</strong> Calibrate pixels_per_meter for accurate speed estimation</li>
            <li><strong>Monitoring:</strong> Regularly check statistics and logs for system health</li>
          </ol>
        </section>

        <section>
          <h2>System Requirements</h2>
          <h3>Hardware</h3>
          <ul>
            <li><strong>CPU:</strong> Intel i5 or better (i7/i9 recommended for multiple cameras)</li>
            <li><strong>RAM:</strong> 8GB minimum, 16GB+ recommended</li>
            <li><strong>GPU:</strong> NVIDIA GPU with CUDA support (optional but recommended)</li>
            <li><strong>Storage:</strong> SSD with sufficient space (100GB+ for large datasets)</li>
            <li><strong>Network:</strong> Stable connection for RTSP streams</li>
          </ul>

          <h3>Software</h3>
          <ul>
            <li>Python 3.8 or higher</li>
            <li>Ultralytics YOLO</li>
            <li>OpenCV (cv2)</li>
            <li>NumPy</li>
            <li>Custom trained YOLO model (bestv4.pt)</li>
          </ul>
        </section>

        <section>
          <h2>Installation and Setup</h2>
          <h3>Quick Start</h3>
          <pre><code># Install dependencies
pip install ultralytics opencv-python numpy

# Clone the repository
git clone https://github.com/kulkarnishub377/Image_collection_vechile_dectetion.git
cd Image_collection_vechile_dectetion

# Configure cameras in config.py
# Run the system
python main.py

# Draw ROIs for each camera
# Press 'q' to quit, 's' for statistics</code></pre>
        </section>

        <section>
          <h2>Technologies Used</h2>
          <div class="blog-tags">
            <span class="blog-tag">YOLO</span>
            <span class="blog-tag">Ultralytics</span>
            <span class="blog-tag">OpenCV</span>
            <span class="blog-tag">Python</span>
            <span class="blog-tag">ByteTrack</span>
            <span class="blog-tag">Kalman Filter</span>
            <span class="blog-tag">RTSP</span>
            <span class="blog-tag">Computer Vision</span>
            <span class="blog-tag">NumPy</span>
            <span class="blog-tag">Multi-threading</span>
          </div>
        </section>

        <section>
          <h2>Future Enhancements</h2>
          <ul>
            <li>Deep learning-based quality assessment</li>
            <li>Automated ANPR integration</li>
            <li>Cloud storage support (AWS S3, Azure Blob)</li>
            <li>Web-based monitoring dashboard</li>
            <li>Mobile app for remote monitoring</li>
            <li>Integration with traffic management systems</li>
          </ul>
        </section>

        <section class="mt-4 bg-light p-3 rounded">
          <h4>Need a Custom Vehicle Detection Solution?</h4>
          <p>If you're working on traffic monitoring, smart city projects, or need a customized vehicle detection system, <a href="../index.html#contact">let's connect</a>! I specialize in building production-ready computer vision systems.</p>
        </section>

      </article>
    </div>
  </main>

        <section>
          <h2>Core Concepts in Video Processing</h2>
          <h3>1. Frame Extraction and Preprocessing</h3>
          <p>Video is essentially a sequence of images (frames). The first step in video processing involves extracting and preprocessing these frames:</p>
          <pre><code>import cv2
import numpy as np

def extract_frames(video_path, fps=30):
    """Extract frames from video at specified fps"""
    cap = cv2.VideoCapture(video_path)
    frame_rate = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(frame_rate / fps)
    
    frames = []
    count = 0
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
            
        if count % frame_interval == 0:
            # Preprocess frame
            frame = cv2.resize(frame, (640, 480))
            frame = cv2.GaussianBlur(frame, (5, 5), 0)
            frames.append(frame)
            
        count += 1
    
    cap.release()
    return frames</code></pre>

          <h3>2. Motion Detection</h3>
          <p>Background subtraction is a fundamental technique for detecting moving objects in video streams:</p>
          <pre><code>def detect_motion(video_path):
    """Detect motion using background subtraction"""
    cap = cv2.VideoCapture(video_path)
    
    # Initialize background subtractor
    bg_subtractor = cv2.createBackgroundSubtractorMOG2(
        history=500, 
        varThreshold=16, 
        detectShadows=True
    )
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # Apply background subtraction
        fg_mask = bg_subtractor.apply(frame)
        
        # Remove noise
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)
        
        # Find contours of moving objects
        contours, _ = cv2.findContours(
            fg_mask, 
            cv2.RETR_EXTERNAL, 
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        # Filter and draw bounding boxes
        for contour in contours:
            if cv2.contourArea(contour) > 500:  # Minimum area threshold
                x, y, w, h = cv2.boundingRect(contour)
                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        
        cv2.imshow('Motion Detection', frame)
        if cv2.waitKey(30) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()</code></pre>
        </section>

        <section>
          <h2>Object Tracking in Video</h2>
          <h3>Using Deep SORT Algorithm</h3>
          <p>For robust multi-object tracking, we use the Deep SORT (Simple Online and Realtime Tracking) algorithm which combines detection with appearance features:</p>
          <ul>
            <li><strong>Detection Phase:</strong> Use YOLOv8 or Faster R-CNN for object detection in each frame</li>
            <li><strong>Tracking Phase:</strong> Use Kalman filter for motion prediction and Hungarian algorithm for data association</li>
            <li><strong>Re-identification:</strong> Deep learning features for handling occlusions</li>
          </ul>
          
          <pre><code>from deep_sort_realtime.deepsort_tracker import DeepSort

def track_objects(video_path):
    """Track multiple objects across video frames"""
    tracker = DeepSort(max_age=30, n_init=3)
    detector = YOLO('yolov8n.pt')  # Load YOLOv8
    
    cap = cv2.VideoCapture(video_path)
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # Detect objects
        results = detector(frame)
        detections = []
        
        for r in results:
            boxes = r.boxes
            for box in boxes:
                x1, y1, x2, y2 = box.xyxy[0]
                conf = box.conf[0]
                cls = box.cls[0]
                
                detections.append([
                    [x1, y1, x2-x1, y2-y1],  # bbox
                    conf,  # confidence
                    int(cls)  # class
                ])
        
        # Update tracker
        tracks = tracker.update_tracks(detections, frame=frame)
        
        # Draw tracking results
        for track in tracks:
            if not track.is_confirmed():
                continue
            track_id = track.track_id
            bbox = track.to_ltrb()
            
            cv2.rectangle(frame, 
                         (int(bbox[0]), int(bbox[1])), 
                         (int(bbox[2]), int(bbox[3])), 
                         (0, 255, 0), 2)
            cv2.putText(frame, f'ID: {track_id}', 
                       (int(bbox[0]), int(bbox[1])-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        cv2.imshow('Tracking', frame)
        if cv2.waitKey(30) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()</code></pre>
        </section>

        <section>
          <h2>Activity Recognition</h2>
          <h3>Using 3D Convolutional Networks</h3>
          <p>Activity recognition involves understanding human actions from video sequences. We use 3D CNNs which can capture temporal information:</p>
          
          <h3>Architecture Overview</h3>
          <ul>
            <li><strong>Input:</strong> Sequences of N frames (e.g., 16 frames)</li>
            <li><strong>Network:</strong> 3D CNN (C3D, I3D, or SlowFast)</li>
            <li><strong>Output:</strong> Activity class (walking, running, jumping, etc.)</li>
          </ul>

          <pre><code>import torch
import torch.nn as nn
from torchvision.models.video import r3d_18

def recognize_activity(video_path, model_path):
    """Recognize human activities in video"""
    # Load pre-trained 3D ResNet
    model = r3d_18(pretrained=True)
    model.eval()
    
    # Activity labels (example)
    activities = [
        'walking', 'running', 'jumping', 
        'sitting', 'standing', 'waving'
    ]
    
    cap = cv2.VideoCapture(video_path)
    frames_buffer = []
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # Preprocess frame
        frame = cv2.resize(frame, (112, 112))
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames_buffer.append(frame)
        
        # Process when we have 16 frames
        if len(frames_buffer) == 16:
            # Prepare input tensor
            clip = np.array(frames_buffer)
            clip = torch.FloatTensor(clip).permute(3, 0, 1, 2)
            clip = clip.unsqueeze(0)  # Add batch dimension
            
            # Predict activity
            with torch.no_grad():
                output = model(clip)
                prediction = torch.argmax(output, dim=1).item()
            
            activity = activities[prediction]
            print(f"Detected Activity: {activity}")
            
            # Keep last 8 frames for sliding window
            frames_buffer = frames_buffer[8:]
    
    cap.release()</code></pre>
        </section>

        <section>
          <h2>Optical Flow Analysis</h2>
          <p>Optical flow captures the pattern of motion between consecutive frames, useful for understanding movement direction and speed:</p>
          
          <pre><code>def compute_optical_flow(video_path):
    """Compute dense optical flow"""
    cap = cv2.VideoCapture(video_path)
    ret, first_frame = cap.read()
    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)
    
    # Create HSV mask for visualization
    hsv_mask = np.zeros_like(first_frame)
    hsv_mask[..., 1] = 255
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # Calculate optical flow (Farneback method)
        flow = cv2.calcOpticalFlowFarneback(
            prev_gray, gray, None, 
            pyr_scale=0.5, levels=3, winsize=15,
            iterations=3, poly_n=5, poly_sigma=1.2, 
            flags=0
        )
        
        # Convert flow to polar coordinates
        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        
        # Visualize flow
        hsv_mask[..., 0] = angle * 180 / np.pi / 2
        hsv_mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)
        flow_rgb = cv2.cvtColor(hsv_mask, cv2.COLOR_HSV2BGR)
        
        cv2.imshow('Optical Flow', flow_rgb)
        if cv2.waitKey(30) & 0xFF == ord('q'):
            break
        
        prev_gray = gray
    
    cap.release()
    cv2.destroyAllWindows()</code></pre>
        </section>

        <section>
          <h2>Real-World Applications</h2>
          <h3>1. Traffic Monitoring System</h3>
          <ul>
            <li>Vehicle counting and classification</li>
            <li>Speed estimation using optical flow</li>
            <li>Accident detection through anomaly detection</li>
            <li>Traffic density analysis</li>
          </ul>

          <h3>2. Surveillance and Security</h3>
          <ul>
            <li>Intrusion detection in restricted areas</li>
            <li>Crowd density estimation</li>
            <li>Suspicious activity recognition</li>
            <li>Face recognition and tracking</li>
          </ul>

          <h3>3. Sports Analytics</h3>
          <ul>
            <li>Player tracking and performance analysis</li>
            <li>Ball trajectory prediction</li>
            <li>Action recognition (goals, fouls, etc.)</li>
            <li>Team formation analysis</li>
          </ul>

          <h3>4. Healthcare Applications</h3>
          <ul>
            <li>Patient fall detection</li>
            <li>Gait analysis for rehabilitation</li>
            <li>Activity monitoring for elderly care</li>
            <li>Gesture recognition for assistive technology</li>
          </ul>
        </section>

        <section>
          <h2>Performance Optimization</h2>
          <h3>GPU Acceleration</h3>
          <p>For real-time video processing, GPU acceleration is essential:</p>
          <pre><code># Using CUDA with OpenCV
import cv2
import torch

# Enable GPU for PyTorch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Use GPU-accelerated OpenCV functions
gpu_frame = cv2.cuda_GpuMat()
gpu_frame.upload(frame)
# Process on GPU
gpu_result = cv2.cuda.resize(gpu_frame, (640, 480))
result = gpu_result.download()</code></pre>

          <h3>Frame Skipping</h3>
          <p>Process every Nth frame for non-critical applications to improve throughput</p>

          <h3>Multi-threading</h3>
          <p>Separate threads for video capture, processing, and display to maximize performance</p>
        </section>

        <section>
          <h2>Best Practices</h2>
          <ol>
            <li><strong>Choose the Right Model:</strong> Balance accuracy and speed based on application requirements</li>
            <li><strong>Preprocessing:</strong> Normalize frames, apply noise reduction, and ensure consistent input size</li>
            <li><strong>Batch Processing:</strong> Process multiple frames together for better GPU utilization</li>
            <li><strong>Memory Management:</strong> Release video capture resources properly to avoid memory leaks</li>
            <li><strong>Error Handling:</strong> Implement robust error handling for corrupted frames or video files</li>
            <li><strong>Benchmarking:</strong> Measure FPS and latency to ensure real-time performance</li>
          </ol>
        </section>

        <section>
          <h2>Tools and Libraries</h2>
          <div class="blog-tags">
            <span class="blog-tag">OpenCV</span>
            <span class="blog-tag">PyTorch</span>
            <span class="blog-tag">TensorFlow</span>
            <span class="blog-tag">YOLO</span>
            <span class="blog-tag">Deep SORT</span>
            <span class="blog-tag">MediaPipe</span>
            <span class="blog-tag">FFmpeg</span>
            <span class="blog-tag">CUDA</span>
            <span class="blog-tag">NumPy</span>
            <span class="blog-tag">Python</span>
          </div>
        </section>

        <section>
          <h2>Conclusion</h2>
          <p>Video processing and analysis with AI opens up countless possibilities across various domains. From surveillance to healthcare, the techniques covered in this article provide a foundation for building sophisticated video analytics applications. The key is to understand your specific requirements and choose the right combination of algorithms and optimizations.</p>
        </section>

        <section class="mt-4 bg-light p-3 rounded">
          <h4>Need Help with Video Analytics?</h4>
          <p>If you're working on video processing projects or need assistance implementing computer vision solutions, <a href="../index.html#contact">get in touch</a>! I specialize in building production-ready video analytics systems.</p>
        </section>

      </article>
    </div>
  </main>

  <footer class="text-center py-4">
    <p class="mb-0">© <span id="copyright-year">2024</span> Shubham Kulkarni</p>
  </footer>
  
  <script src="../js/script.js"></script>
  <script src="../js/blog.js"></script>
  <script>try{document.getElementById('copyright-year').textContent=new Date().getFullYear()}catch(e){};</script>
</body>
</html>
